# Evaluation

This directory contains scripts implementing various evaluations to be performed on the Semantic Spuriosity Scores generated by SemScore. An example of how evaluation is run can be found in `demo/run_evaluation.ipynb`.

## Aggregate raw outputs

After getting raw outputs from running the various pipelines, aggregate the scores to perform further evaluations and analysis using:

```bash
python aggregate_scores.py --config configs/aggregated_scores_config.yaml
```

## Functions for evaluation

After aggregating the scores, several types of evaluations can be done. Evaluation tasks can be selected and configurations can be modified in `configs/class_analysis.yaml`

### Top and bottom n classes using SSS

This function enables users to check the classes with highest and lowest SSS. It takes in the aggregated outputs from a specific model and the top and bottom n number of classes users wish to view. Resulting CSVs will be saved to `outputs/class_based_scores`. Users will be able to see these classes and the saliency methods that yielded their respective SSS scores.

### Checking SSS for specific classes

This function enables users to check the SSS for specific classes that they are interested in. It takes in the aggregated outputs from a specific model and the classes that users are interested in. Resulting CSVs will be saved to `outputs/class_based_scores`. Users will be able to see the SSS of these classes with the different saliency maps available.

### Confidence thresholding on raw outputs

This function enables users to check SSS based on different confidence thresholds, potentially allowing users to identify reasons for their models having low confidence. It takes in the raw outputs from the pipelines and performs aggregation, giving the aggregated SSS scores for that model across different saliency methods. Resulting CSVs will be saved to `outputs/threshold_analysis`.

### SSS thresholding on raw outputs

This function enables users to view SSS based on different  thresholds, allowing users to discard low scores in the saliency maps. It takes in the raw outputs from the pipelines and performs aggregation, giving the aggregated and thresholded SSS scores for that model across different saliency methods. Results will be saved to `outputs/threshold_analysis`.

### Vision transformer layer analysis

This function enables users to obtain SSS across different layers for ViTs. It takes in the raw outputs from the models and the different layers that users are keen to analyze. It then outputs SSS for the different layers, across the various saliency methods. Results will be saved to `outputs/vit_layer_analysis`.
